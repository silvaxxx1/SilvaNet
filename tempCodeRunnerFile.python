import cupy as cp
import numpy as np
import matplotlib.pyplot as plt

# Activation Functions
class ActivationFunctions:
    @staticmethod
    def relu(x):
        return cp.maximum(0, x)

    @staticmethod
    def sigmoid(x):
        return 1 / (1 + cp.exp(-x))

    @staticmethod
    def tanh(x):
        return cp.tanh(x)
    
    @staticmethod
    def softmax(x):
        exp_x = cp.exp(x - cp.max(x, axis=1, keepdims=True))
        return exp_x / cp.sum(exp_x, axis=1, keepdims=True)

# Derivatives
class ActivationDerivatives:
    @staticmethod
    def back_relu(x):
        return cp.where(x > 0, 1, 0)

    @staticmethod
    def back_sigmoid(x):
        return x * (1 - x)
    
    @staticmethod
    def back_tanh(x):
        return 1 - cp.tanh(x) ** 2

# Loss Functions
class LossFunctions:
    @staticmethod
    def binary_cross_entropy(y_true, y_pred):
        epsilon = 1e-15
        y_pred = cp.clip(y_pred, epsilon, 1 - epsilon)
        return -cp.mean(y_true * cp.log(y_pred) + (1 - y_true) * cp.log(1 - y_pred))
    
    @staticmethod
    def binary_cross_entropy_derivative(y_true, y_pred):
        epsilon = 1e-15
        y_pred = cp.clip(y_pred, epsilon, 1 - epsilon)
        return y_pred - y_true

# Neural Network Class
class NeuralNetwork:
    def __init__(self, layer_dims, activation=['relu', 'sigmoid'], learning_rate=0.01, init_type='he'):
        self.layer_dims = layer_dims
        self.activation = activation
        self.learning_rate = learning_rate
        self.params = self._initialize_params(init_type)

    def _initialize_params(self, init_type):
        params = {}
        L = len(self.layer_dims)
        for l in range(1, L):
            prev, curr = self.layer_dims[l - 1], self.layer_dims[l]
            if init_type == "he":
                params[f'W{l}'] = cp.random.randn(prev, curr) * cp.sqrt(2. / prev)
            elif init_type == "xavier":
                params[f'W{l}'] = cp.random.randn(prev, curr) * cp.sqrt(1. / prev)
            else:
                params[f'W{l}'] = cp.random.randn(prev, curr)
            params[f'b{l}'] = cp.zeros((1, curr))
        return params

    def forward(self, X):
        L = len(self.params) // 2
        A = X
        self.cache = {0: X}
        for l in range(1, L + 1):
            Z = cp.dot(A, self.params[f'W{l}']) + self.params[f'b{l}']
            A = ActivationFunctions.relu(Z) if self.activation[l - 1] == 'relu' else ActivationFunctions.sigmoid(Z)
            self.cache[l] = A
        return A

    def backward(self, X, y):
        L = len(self.params) // 2
        m = X.shape[0]
        dW, db = {}, {}
        dZ = self.cache[L] - y.reshape(-1, 1)
        for l in range(L, 0, -1):
            dW[l] = cp.dot(self.cache[l - 1].T, dZ) / m
            db[l] = cp.sum(dZ, axis=0, keepdims=True) / m
            if l > 1:
                dZ = cp.dot(dZ, self.params[f'W{l}'].T) * ActivationDerivatives.back_relu(self.cache[l - 1])
        for l in range(1, L + 1):
            self.params[f'W{l}'] -= self.learning_rate * dW[l]
            self.params[f'b{l}'] -= self.learning_rate * db[l]

    def train(self, X, y, epochs=1000, verbose=True):
        loss_history = []
        for epoch in range(epochs):
            self.forward(X)
            self.backward(X, y)
            loss = LossFunctions.binary_cross_entropy(y.reshape(-1, 1), self.cache[len(self.layer_dims) - 1])
            loss_history.append(loss)
            if verbose and epoch % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs} - Loss: {loss:.4f}")
        return loss_history

    def predict(self, X):
        return self.forward(X)

    def plot_loss(self, loss_history):
        plt.plot(cp.asnumpy(cp.array(loss_history)))
        plt.title("Loss over epochs")
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.show()

# Example Usage
X = cp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
y = cp.array([1, 0, 0, 1])

nn = NeuralNetwork(layer_dims=[3, 4, 5, 1], activation=['relu', 'sigmoid'], learning_rate=0.01)
loss_history = nn.train(X, y, epochs=1000)
nn.plot_loss(loss_history)
X_test = cp.array([[1, 2, 3], [4, 5, 6]])
print("Predictions:", nn.predict(X_test))
