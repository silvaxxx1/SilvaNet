{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the first dev notebook for building a deep learning library from scarcth in native numpy \n",
    "\n",
    "the overall goal : grasp a deep understanding of the main building blocks in DL systems\n",
    "\n",
    "this notebook goal : design a simple nn to predict on mack random data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "x = np.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9]])\n",
    "\n",
    "y = np.array([1,0,0])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](images/nn.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02008805 -0.48709471  0.49338108 -0.0247763  -2.13042258  0.65129621]\n",
      " [ 0.52289062 -1.10864025 -0.24051315 -0.64075336 -0.27325533 -0.4135334 ]\n",
      " [ 0.57556149 -1.13897708  1.73982202  0.15234672 -0.2840554  -0.55961484]]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "(3, 6)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "# lets decide the number of hiiden neurons we want\n",
    "# we only we use on hidden  layer\n",
    "n_neurons = 6\n",
    "\n",
    "num_of_example , num_of_features = x.shape \n",
    "\n",
    "# initialize the weights and biases\n",
    "W1 = np.random.randn(num_of_features, n_neurons)\n",
    "b1 = np.zeros((n_neurons,)) \n",
    "\n",
    "print(W1)\n",
    "print(b1)\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.79255376  -6.12130646   5.23182085  -0.84924286  -3.52909944\n",
      "   -1.85461511]\n",
      " [ 10.14817425 -14.3254426   11.20989071  -2.38879168 -11.59229938\n",
      "   -2.8201712 ]\n",
      " [ 16.50379475 -22.52957874  17.18796057  -3.9283405  -19.65549931\n",
      "   -3.78572728]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "# the forward pass (linear transformation)\n",
    "\n",
    "z1 = np.dot(x, W1) + b1\n",
    "\n",
    "print(z1)\n",
    "print(z1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9989846  -0.99999036  0.99994289 -0.69067372 -0.99828083 -0.9521787 ]\n",
      " [ 1.         -1.          1.         -0.98330786 -1.         -0.99292183]\n",
      " [ 1.         -1.          1.         -0.99922599 -1.         -0.99897064]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "# activation function\n",
    "## adding the non linearity to the linear transformation\n",
    "\n",
    "a1 = np.tanh(z1)\n",
    "\n",
    "print(a1)\n",
    "print(a1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.33424614]\n",
      " [-0.28318377]\n",
      " [ 0.3744034 ]\n",
      " [-0.89129719]\n",
      " [ 1.64891485]\n",
      " [-0.60307707]]\n",
      "[0.]\n",
      "(6, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# the output of the network\n",
    "out_layer_neurons = 1\n",
    "W2 = np.random.randn(n_neurons, out_layer_neurons)\n",
    "b2 = np.zeros((out_layer_neurons,))\n",
    "\n",
    "print(W2)\n",
    "print(b2)\n",
    "print(W2.shape)\n",
    "print(b2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.13157567]\n",
      " [-0.85034588]\n",
      " [-0.83251021]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# second forward pass\n",
    "z2 = np.dot(a1 , W2) + b2 \n",
    "print(z2)\n",
    "print(z2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24387043]\n",
      " [0.29936031]\n",
      " [0.30311456]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## adding the non linearity to the linear transformation \n",
    "## this type we will use sigmoid activation function since this is a binary classification problem \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(a2)\n",
    "print(a2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6448607372060217\n"
     ]
    }
   ],
   "source": [
    "## next we must calculate the loss\n",
    "## we will use binary cross entropy loss function , again becuse this is a binary classification problem\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.10053807  1.32252466  1.32252466]\n",
      " [-3.34045623  1.42726712  1.42726712]\n",
      " [-3.29908267  1.43495609  1.43495609]]\n"
     ]
    }
   ],
   "source": [
    "## now we must optimize (minmize this loss function) by the power of chain rule and gradient descent \n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -((y_true / y_pred) - ((1 - y_true) / (1 - y_pred)))\n",
    "\n",
    "d_loss_d_a2 = binary_cross_entropy_derivative(y, a2)\n",
    "print(d_loss_d_a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18439765]\n",
      " [0.20974371]\n",
      " [0.21123612]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x*(1 - x) \n",
    "\n",
    "d_a2_d_z2 = sigmoid_derivative(a2)\n",
    "\n",
    "print(d_a2_d_z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75612957  0.24387043  0.24387043]\n",
      " [-0.70063969  0.29936031  0.29936031]\n",
      " [-0.69688544  0.30311456  0.30311456]]\n"
     ]
    }
   ],
   "source": [
    "out_layer_grad = d_loss_d_a2 * d_a2_d_z2 \n",
    "print(out_layer_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. a2: [[-0.75612957]\n",
      " [ 0.29936031]\n",
      " [ 0.30311456]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## TO simply we can use the this function and get the same result :\n",
    "\n",
    "def out_layer_grad(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    \n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true)\n",
    "\n",
    "# Reshape y to match a2\n",
    "y = y.reshape(-1, 1)  # Ensure y has shape (3,1)\n",
    "\n",
    "# Compute the gradient of the loss with respect to a2\n",
    "grad_a2 = out_layer_grad(y, a2)\n",
    "\n",
    "print(\"Gradient w.r.t. a2:\", grad_a2)\n",
    "print(grad_a2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAME RESULT !! by the power of math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71288511]\n",
      " [ 0.2740577 ]\n",
      " [ 0.27688598]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## lets move to the backprop of the hidden layer:\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2 \n",
    "\n",
    "\n",
    "grad_z2 = tanh_derivative(a2) * grad_a2\n",
    "\n",
    "print(grad_z2)\n",
    "print(grad_z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05373919]\n",
      " [ 0.05397819]\n",
      " [-0.05396691]\n",
      " [-0.01792792]\n",
      " [ 0.05357195]\n",
      " [ 0.04335839]]\n",
      "(6, 1)\n",
      "[[-0.05398048]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# now we need  to calcukate the grad for the parametrs in the hidden layer (W2 , B2)\n",
    "dW2 = np.dot( a1.T, grad_z2) / num_of_example\n",
    "db2 = np.sum(grad_z2, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "print(dW2)\n",
    "print(dW2.shape)\n",
    "\n",
    "print(db2)\n",
    "print(db2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95116421  0.2018775  -0.26690661  0.6353925  -1.17548684  0.42992467]\n",
      " [-0.36566043 -0.07760869  0.10260814 -0.24426686  0.45189781 -0.16527792]\n",
      " [-0.36943405 -0.07840962  0.10366705 -0.2467877   0.4565614  -0.16698359]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "grad_a1 = np.dot(grad_z2, W2.T)\n",
    "\n",
    "print(grad_a1)\n",
    "print(grad_a1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4000827   0.08478461 -0.11210368  0.4078584  -0.49496814  0.19400912]\n",
      " [-0.153568   -0.03259366  0.04309278 -0.10521511  0.18978549 -0.07016341]\n",
      " [-0.15515282 -0.03293003  0.0435375  -0.10376674  0.19174407 -0.07023883]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "dz1  = grad_a1 * tanh_derivative(a1)\n",
    "print(dz1)\n",
    "print(dz1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05373919]\n",
      " [ 0.05397819]\n",
      " [-0.05396691]\n",
      " [-0.01792792]\n",
      " [ 0.05357195]\n",
      " [ 0.04335839]]\n",
      "(6, 1)\n",
      "[[-0.05398048]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# now we need  to calcukate the grad for the parametrs in the hidden layer (W2 , B2)\n",
    "dW1 = np.dot( x.T, dz1) / num_of_example\n",
    "db1 = np.sum(dz1, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "print(dW2)\n",
    "print(dW2.shape)\n",
    "\n",
    "print(db2)\n",
    "print(db2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new W1: [[ 1.02442225 -0.48617437  0.49216432 -0.02231174 -2.13577719  0.65322393]\n",
      " [ 0.52692028 -1.10778412 -0.24164501 -0.63895171 -0.27823181 -0.41178437]\n",
      " [ 0.5792866  -1.13818515  1.73877508  0.15348544 -0.28865375 -0.5580445 ]]\n",
      "new b1: [-3.04539601e-04 -6.42030881e-05  8.49113114e-05 -6.62921834e-04\n",
      "  3.78128603e-04 -1.78689580e-04]\n",
      "new W2: [[-1.33370875]\n",
      " [-0.28372355]\n",
      " [ 0.37494307]\n",
      " [-0.89111792]\n",
      " [ 1.64837913]\n",
      " [-0.60351065]]\n",
      "new bw: [0.0005398]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "\n",
    "W1 -= lr * dW1\n",
    "b1 -= lr * db1.ravel()  # Fix here\n",
    "W2 -= lr * dW2\n",
    "b2 -= lr * db2.ravel()  # No need to fix since b2 is already (1,)\n",
    "\n",
    "\n",
    "print(\"new W1:\", W1)\n",
    "print(\"new b1:\", b1)\n",
    "print(\"new W2:\", W2)\n",
    "print(\"new bw:\", b2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.7103104629722242\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z1 = np.dot(x, W1) + b1\n",
    "a1 = np.tanh(z1)\n",
    "\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "# Compute new loss\n",
    "loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "print(\"New loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7103104629722242\n",
      "Epoch 10, Loss: 0.7130924490207929\n",
      "Epoch 20, Loss: 0.7158640169142508\n",
      "Epoch 30, Loss: 0.7186140713945002\n",
      "Epoch 40, Loss: 0.7213306956938573\n",
      "Epoch 50, Loss: 0.7240011629424207\n",
      "Epoch 60, Loss: 0.7266118963590932\n",
      "Epoch 70, Loss: 0.7291483650060098\n",
      "Epoch 80, Loss: 0.7315948999691964\n",
      "Epoch 90, Loss: 0.7339344128948743\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 100  # Number of training iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "    # Backpropagation\n",
    "    grad_a2 = out_layer_grad(y, a2)\n",
    "    grad_z2 = grad_a2 * sigmoid(a2) * (1 - sigmoid(a2))\n",
    "\n",
    "    dW2 = np.dot(a1.T, grad_z2) / num_of_example\n",
    "    db2 = np.sum(grad_z2, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "    grad_z1 = np.dot(grad_z2, W2.T) * tanh_derivative(a1)\n",
    "\n",
    "    dW1 = np.dot(x.T, grad_z1) / num_of_example\n",
    "    db1 = np.sum(grad_z1, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "    # Parameter updates\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1.ravel()\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2.ravel()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 weight shape: torch.Size([6, 3])\n",
      "fc1 bias shape: torch.Size([6])\n",
      "fc2 weight shape: torch.Size([1, 6])\n",
      "fc2 bias shape: torch.Size([1])\n",
      "Epoch 0, Loss: 0.8977\n",
      "Epoch 10, Loss: 0.8208\n",
      "Epoch 20, Loss: 0.7587\n",
      "Epoch 30, Loss: 0.7182\n",
      "Epoch 40, Loss: 0.6902\n",
      "Epoch 50, Loss: 0.6694\n",
      "Epoch 60, Loss: 0.6528\n",
      "Epoch 70, Loss: 0.6385\n",
      "Epoch 80, Loss: 0.6252\n",
      "Epoch 90, Loss: 0.6119\n",
      "Final predictions: tensor([[0.4061],\n",
      "        [0.3600],\n",
      "        [0.3600]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "tensor_x = torch.tensor(x , dtype=torch.float32)\n",
    "tensor_y = torch.tensor(y , dtype = torch.float32)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 6)  # input: 3, output: 6\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(6, 1)  # input: 6, output: 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.fc1(x)         # automatically: z1 = x * W1^T + b1, shape: (3,6)\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = self.fc2(a1)        # shape: (3,1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2 \n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.randn(3, 6)\n",
    "b1 = np.zeros((6,))\n",
    "W2 = np.random.randn(6, 1)\n",
    "b2 = np.zeros((1,))\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "model.fc1.weight.data = torch.tensor(W1.T, dtype=torch.float32)  # shape: (6,3)\n",
    "model.fc1.bias.data = torch.tensor(b1, dtype=torch.float32)        # shape: (6,)\n",
    "\n",
    "# For fc2: your NumPy W2 is (6,1), and PyTorch expects (1,6)\n",
    "model.fc2.weight.data = torch.tensor(W2.T, dtype=torch.float32)  # shape: (1,6)\n",
    "model.fc2.bias.data = torch.tensor(b2, dtype=torch.float32)        # shape: (1,)\n",
    "\n",
    "# Verify parameter shapes:\n",
    "print(\"fc1 weight shape:\", model.fc1.weight.data.shape)  # (6,3)\n",
    "print(\"fc1 bias shape:\", model.fc1.bias.data.shape)      # (6,)\n",
    "print(\"fc2 weight shape:\", model.fc2.weight.data.shape)  # (1,6)\n",
    "print(\"fc2 bias shape:\", model.fc2.bias.data.shape)      # (1,)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (using the same hyperparameters)\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()         # Reset gradients\n",
    "    output = model(tensor_x)        # Forward pass, output shape: (3,1)\n",
    "    loss = criterion(output, tensor_y)\n",
    "    loss.backward()               # Backward pass (computes gradients automatically)\n",
    "    optimizer.step()              # Update parameters\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final forward pass to see predictions\n",
    "with torch.no_grad():\n",
    "    final_output = model(tensor_x)\n",
    "    print(\"Final predictions:\", final_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
