{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp \n",
    "\n",
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "            [10, 11, 12]])\n",
    "\n",
    "y = cp.array([1,0,0, 1])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return cp.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(cp.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        sig = ActivationFunctions.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return cp.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - cp.tanh(x)**2\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-2. -1.  0.  1.  2.]\n",
      "ReLU Output: [0. 0. 0. 1. 2.]\n",
      "ReLU Derivative: [0. 0. 0. 1. 1.]\n",
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "tanh Output: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "tanh Derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Create a test input (CuPy array)\n",
    "x = cp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ✅ Test ReLU\n",
    "relu_output = ActivationFunctions.relu(x)\n",
    "relu_derivative_output = ActivationFunctions.relu_derivative(x)\n",
    "\n",
    "# ✅ Test Sigmoid\n",
    "sigmoid_output = ActivationFunctions.sigmoid(x)\n",
    "sigmoid_derivative_output = ActivationFunctions.sigmoid_derivative(x)\n",
    "\n",
    "# ✅ Test Tanh\n",
    "tanh_output = ActivationFunctions.tanh(x)\n",
    "tanh_derivative_output = ActivationFunctions.tanh_derivative(x)\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"Input:\", x)\n",
    "print(\"ReLU Output:\", relu_output)\n",
    "print(\"ReLU Derivative:\", relu_derivative_output)\n",
    "print(\"Sigmoid Output:\", sigmoid_output)\n",
    "print(\"Sigmoid Derivative:\", sigmoid_derivative_output)\n",
    "print(\"tanh Output:\", tanh_output)\n",
    "print(\"tanh Derivative:\", tanh_derivative_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp  # Assuming you're using CuPy for GPU acceleration\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 initializer: str = \"he\"):\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights\n",
    "        if initializer == 'he':\n",
    "            scale = cp.sqrt(2.0 / in_features)  # Fixed denominator\n",
    "        elif initializer == 'xavier':\n",
    "            scale = cp.sqrt(1.0 / in_features)  # Fixed denominator\n",
    "        else:  # Plain initialization\n",
    "            scale = 1.0\n",
    "\n",
    "        self.weights = cp.random.randn(out_features, in_features) * scale\n",
    "        self.bias = cp.zeros((out_features,)) if bias else None\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dweights = cp.zeros_like(self.weights)\n",
    "        self.dbias = cp.zeros_like(self.bias) if bias else None\n",
    "\n",
    "    def forward(self, x: cp.ndarray) -> cp.ndarray:\n",
    "        self.x = x \n",
    "        return cp.dot(x, self.weights.T) + (self.bias if self.bias is not None else 0)  # Ensures correct shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4.65469258   1.27288809]\n",
      " [ -8.42404719   2.15758172]\n",
      " [-12.19340181   3.04227535]\n",
      " [-15.96275642   3.92696897]]\n"
     ]
    }
   ],
   "source": [
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "            [10, 11, 12]])\n",
    "\n",
    "layer = Linear(3, 2)  # 3 input features, 2 output features\n",
    "out  = layer.forward(x)\n",
    "\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, initializer: str = \"he\"):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights\n",
    "        if initializer == 'he':\n",
    "            scale = cp.sqrt(2.0 / in_features)\n",
    "        elif initializer == 'xavier':\n",
    "            scale = cp.sqrt(1.0 / in_features)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        self.weights = cp.random.randn(out_features, in_features) * scale\n",
    "        self.bias = cp.zeros((out_features,)) if bias else None\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dweights = cp.zeros_like(self.weights)\n",
    "        self.dbias = cp.zeros_like(self.bias) if bias else None\n",
    "\n",
    "    def forward(self, x: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\" Forward pass: Computes Y = XW^T + b \"\"\"\n",
    "        self.x = x  # Store input for backprop\n",
    "        return cp.dot(x, self.weights.T) + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "    def backward(self, upstream_grad: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation)\n",
    "\n",
    "        Args:\n",
    "            upstream_grad: Gradient from subsequent layer, shape (batch_size, output_dim)\n",
    "\n",
    "        Returns:\n",
    "            Gradient with respect to input, shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Compute gradients\n",
    "        self.dweights = cp.dot(upstream_grad.T, self.x)  # (out_features, in_features)\n",
    "        if self.bias is not None:\n",
    "            self.dbias = cp.sum(upstream_grad, axis=0)    # (out_features,)\n",
    "\n",
    "        # Compute gradient for input\n",
    "        dx = cp.dot(upstream_grad, self.weights)         # (batch_size, in_features)\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update(self, learning_rate: float):\n",
    "        \"\"\"Update weights using computed gradients\"\"\"\n",
    "        self.weights -= learning_rate * self.dweights\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate * self.dbias\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"Return weights and biases\"\"\"\n",
    "        return {'weights': self.weights, 'bias': self.bias}\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        \"\"\"Return current gradients\"\"\"\n",
    "        return {'dweights': self.dweights, 'dbias': self.dbias}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      " [[ -4.87847309   0.66462138]\n",
      " [-10.52705953   1.35894429]\n",
      " [-16.17564598   2.0532672 ]\n",
      " [-21.82423242   2.74759012]]\n",
      "\n",
      "Gradient w.r.t Input:\n",
      " [[ 0.46778272  1.01007504  3.05260214]\n",
      " [-0.12136245 -0.21295474 -0.61117007]\n",
      " [-0.14084134 -0.21826389 -0.60295544]\n",
      " [ 0.00515197  0.10965762  0.39644441]]\n",
      "\n",
      "Weight Gradient:\n",
      " [[ 3.90744476  2.56820989  1.22897503]\n",
      " [28.01494466 30.66171602 33.30848737]]\n",
      "\n",
      "Bias Gradient:\n",
      " [-1.33923487  2.64677135]\n",
      "\n",
      "Updated Weights:\n",
      " [[-0.21507349 -0.44379736 -1.30103759]\n",
      " [-0.28958477 -0.25804497 -0.14078077]]\n",
      "\n",
      "Updated Bias:\n",
      " [ 0.01339235 -0.02646771]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3) -> batch_size=4, input_dim=3\n",
    "\n",
    "# Create Linear layer: input_dim=3, output_dim=2\n",
    "linear_layer = Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "# Forward pass\n",
    "output = linear_layer.forward(x)\n",
    "print(\"Forward Output:\\n\", output)\n",
    "\n",
    "# Define upstream gradient (random values for testing)\n",
    "upstream_grad = cp.random.randn(4, 2)  # Same shape as output\n",
    "\n",
    "# Backward pass\n",
    "grad_input = linear_layer.backward(upstream_grad)\n",
    "print(\"\\nGradient w.r.t Input:\\n\", grad_input)\n",
    "\n",
    "# Print gradients of weights and bias\n",
    "print(\"\\nWeight Gradient:\\n\", linear_layer.dweights)\n",
    "print(\"\\nBias Gradient:\\n\", linear_layer.dbias)\n",
    "\n",
    "# Update parameters\n",
    "learning_rate = 0.01\n",
    "linear_layer.update(learning_rate)\n",
    "\n",
    "# Print updated weights and bias\n",
    "print(\"\\nUpdated Weights:\\n\", linear_layer.weights)\n",
    "print(\"\\nUpdated Bias:\\n\", linear_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, activation: str):\n",
    "        self.activation = activation\n",
    "        self.x = None  # Store input for backprop\n",
    "\n",
    "        # Set activation function and its derivative\n",
    "        if activation == \"relu\":\n",
    "            self.func = ActivationFunctions.relu\n",
    "            self.derivative = ActivationFunctions.relu_derivative\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.func = ActivationFunctions.sigmoid\n",
    "            self.derivative = ActivationFunctions.sigmoid_derivative\n",
    "        elif activation == \"tanh\":\n",
    "            self.func = ActivationFunctions.tanh\n",
    "            self.derivative = ActivationFunctions.tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass: Apply activation function \"\"\"\n",
    "        self.x = x  # Store for backward pass\n",
    "        return self.func(x)\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        \"\"\" Backward pass: Apply activation derivative \"\"\"\n",
    "        return upstream_grad * self.derivative(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated Output:\n",
      " [[-0.90983665 -0.52384797]\n",
      " [-0.99214963 -0.8567783 ]\n",
      " [-0.99934236 -0.96262925]\n",
      " [-0.99994509 -0.99064311]]\n",
      "\n",
      "Gradient w.r.t Input:\n",
      " [[ 0.10420543 -0.15336635  0.17225569]\n",
      " [ 0.04014839 -0.06070295  0.04583223]\n",
      " [ 0.01057647 -0.01566041  0.01628363]\n",
      " [ 0.00072666 -0.00108212  0.00104036]]\n"
     ]
    }
   ],
   "source": [
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3)\n",
    "\n",
    "# Create layers\n",
    "linear_layer = Linear(in_features=3, out_features=2, bias=True)\n",
    "activation_layer = Activation(\"tanh\")  # Can be \"sigmoid\" or \"tanh\"\n",
    "\n",
    "# Forward pass\n",
    "linear_output = linear_layer.forward(x)\n",
    "activated_output = activation_layer.forward(linear_output)\n",
    "\n",
    "print(\"Activated Output:\\n\", activated_output)\n",
    "\n",
    "# Backward pass (simulating loss gradient)\n",
    "upstream_grad = cp.random.randn(4, 2)\n",
    "grad_activated = activation_layer.backward(upstream_grad)\n",
    "grad_input = linear_layer.backward(grad_activated)\n",
    "\n",
    "print(\"\\nGradient w.r.t Input:\\n\", grad_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"\n",
    "        A simple sequential model to stack layers.\n",
    "\n",
    "        Args:\n",
    "            *layers: A list of layers (Linear, Activation, etc.)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through all layers \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        \"\"\" Backward pass through all layers in reverse order \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            upstream_grad = layer.backward(upstream_grad)\n",
    "        return upstream_grad\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\" Update weights of layers that have parameters (Linear layers) \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"update\"):\n",
    "                layer.update(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:\n",
      " [[4.02194951e-01 7.33736526e-01]\n",
      " [5.24591193e-02 9.58006111e-01]\n",
      " [5.84850073e-03 9.94863931e-01]\n",
      " [7.01209019e-04 9.99399263e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3)\n",
    "\n",
    "# Create a simple feedforward network\n",
    "model = Sequential(\n",
    "    Linear(in_features=3, out_features=5, bias=True),\n",
    "    Activation(\"relu\"),\n",
    "    Linear(in_features=5, out_features=2, bias=True),\n",
    "    Activation(\"sigmoid\")\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Model Output:\\n\", output)\n",
    "\n",
    "# Backward pass (simulating loss gradient)\n",
    "upstream_grad = cp.random.randn(4, 2)\n",
    "model.backward(upstream_grad)\n",
    "\n",
    "# Update weights\n",
    "model.update(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        # Binary Cross-Entropy loss: -target * log(output) - (1 - target) * log(1 - output)\n",
    "        epsilon = 1e-15  # To avoid log(0) errors\n",
    "        output = cp.clip(output, epsilon, 1 - epsilon)\n",
    "        loss = -cp.mean(target * cp.log(output) + (1 - target) * cp.log(1 - output))\n",
    "        self.output = output\n",
    "        self.target = target\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        # Gradient of BCE loss with respect to output\n",
    "        grad_input = (self.output - self.target) / (self.output * (1 - self.output))\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.9514\n",
      "Epoch 100/1000, Loss: 0.5961\n",
      "Epoch 200/1000, Loss: 0.5877\n",
      "Epoch 300/1000, Loss: 0.5874\n",
      "Epoch 400/1000, Loss: 0.5872\n",
      "Epoch 500/1000, Loss: 0.5870\n",
      "Epoch 600/1000, Loss: 0.5869\n",
      "Epoch 700/1000, Loss: 0.5869\n",
      "Epoch 800/1000, Loss: 0.5869\n",
      "Epoch 900/1000, Loss: 0.5869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Setup\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3)\n",
    "\n",
    "# Target labels for binary classification (simulated)\n",
    "y = cp.array([[1], [0], [1], [0]])  # (4, 1)\n",
    "\n",
    "# Create a simple feedforward network\n",
    "model = Sequential(\n",
    "    Linear(in_features=3, out_features=5, bias=True),\n",
    "    Activation(\"relu\"),\n",
    "    Linear(in_features=5, out_features=1, bias=True),  # Single output for binary classification\n",
    "    Activation(\"sigmoid\")  # Sigmoid for binary output\n",
    ")\n",
    "\n",
    "# Initialize BCE loss\n",
    "loss_fn = BCE()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = model.forward(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn.forward(output, y)\n",
    "\n",
    "    # Backward pass\n",
    "    upstream_grad = loss_fn.backward()  # Gradient of loss w.r.t. output\n",
    "    model.backward(upstream_grad)\n",
    "\n",
    "    # Update weights\n",
    "    model.update(learning_rate)\n",
    "\n",
    "    # Print loss for every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SilvaXnet_cuda11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
