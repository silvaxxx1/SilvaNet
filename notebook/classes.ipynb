{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp \n",
    "\n",
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "            [10, 11, 12]])\n",
    "\n",
    "y = cp.array([1,0,0, 1])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return cp.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(cp.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        sig = ActivationFunctions.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-2. -1.  0.  1.  2.]\n",
      "ReLU Output: [0. 0. 0. 1. 2.]\n",
      "ReLU Derivative: [0. 0. 0. 1. 1.]\n",
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Create a test input (CuPy array)\n",
    "x = cp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ✅ Test ReLU\n",
    "relu_output = ActivationFunctions.relu(x)\n",
    "relu_derivative_output = ActivationFunctions.relu_derivative(x)\n",
    "\n",
    "# ✅ Test Sigmoid\n",
    "sigmoid_output = ActivationFunctions.sigmoid(x)\n",
    "sigmoid_derivative_output = ActivationFunctions.sigmoid_derivative(x)\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"Input:\", x)\n",
    "print(\"ReLU Output:\", relu_output)\n",
    "print(\"ReLU Derivative:\", relu_derivative_output)\n",
    "print(\"Sigmoid Output:\", sigmoid_output)\n",
    "print(\"Sigmoid Derivative:\", sigmoid_derivative_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.24999936504123763\n",
      "Epoch 100, Loss: 0.19586890365681792\n",
      "Epoch 200, Loss: 0.18862010694466977\n",
      "Epoch 300, Loss: 0.18764942278779112\n",
      "Epoch 400, Loss: 0.18751942581429387\n",
      "Epoch 500, Loss: 0.18750200870455197\n",
      "Epoch 600, Loss: 0.18749966581513067\n",
      "Epoch 700, Loss: 0.18749934146376127\n",
      "Epoch 800, Loss: 0.18749928732143345\n",
      "Epoch 900, Loss: 0.1874992692721445\n",
      "\n",
      "Predictions after training:\n",
      " [[0.24998795]\n",
      " [0.24998825]\n",
      " [0.24999018]\n",
      " [0.24999078]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W = cp.random.randn(input_size, output_size) * 0.01  # Weight\n",
    "        self.b = cp.zeros((1, output_size))  # Bias\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.Z = cp.dot(X, self.W) + self.b\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ, learning_rate):\n",
    "        m = self.X.shape[0]\n",
    "        dW = cp.dot(self.X.T, dZ) / m\n",
    "        db = cp.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dX = cp.dot(dZ, self.W.T)\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        return dX\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, Z):\n",
    "        self.A = cp.maximum(0, Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = dA * (self.A > 0)  # Derivative of ReLU\n",
    "        return dZ\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        # Initialize layers\n",
    "        self.layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(Linear(prev_size, hidden_size))\n",
    "            self.layers.append(ReLU())\n",
    "            prev_size = hidden_size\n",
    "        self.layers.append(Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass through the network\n",
    "        self.activations = []\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "            self.activations.append(Z)\n",
    "        return Z\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Mean Squared Error loss\n",
    "        loss = cp.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.compute_loss(y_pred, y_true)\n",
    "\n",
    "        # Backpropagation\n",
    "        dZ = y_pred - y_true  # Derivative of MSE loss with respect to output\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            layer = self.layers[i]\n",
    "            if isinstance(layer, Linear):\n",
    "                dZ = layer.backward(dZ, learning_rate)\n",
    "            elif isinstance(layer, ReLU):\n",
    "                dZ = layer.backward(dZ)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward and backward pass\n",
    "            loss = self.backward(X, y, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Toy dataset (X: input features, y: target labels)\n",
    "    X = cp.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # AND gate input\n",
    "    y = cp.array([[0], [0], [0], [1]])  # AND gate output\n",
    "\n",
    "    # Initialize the neural network with 2 input nodes, two hidden layers of 4 nodes each, and 1 output node\n",
    "    nn = NeuralNetwork(input_size=2, hidden_sizes=[4, 4], output_size=1)\n",
    "\n",
    "    # Train the network\n",
    "    nn.train(X, y, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "    # Test the network after training\n",
    "    y_pred = nn.forward(X)\n",
    "    print(\"\\nPredictions after training:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SilvaXnet_cuda11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
