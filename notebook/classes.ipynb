{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp \n",
    "\n",
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "            [10, 11, 12]])\n",
    "\n",
    "y = cp.array([1,0,0, 1])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return cp.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(cp.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        sig = ActivationFunctions.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return cp.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - cp.tanh(x)**2\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-2. -1.  0.  1.  2.]\n",
      "ReLU Output: [0. 0. 0. 1. 2.]\n",
      "ReLU Derivative: [0. 0. 0. 1. 1.]\n",
      "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Sigmoid Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "tanh Output: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
      "tanh Derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Create a test input (CuPy array)\n",
    "x = cp.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ✅ Test ReLU\n",
    "relu_output = ActivationFunctions.relu(x)\n",
    "relu_derivative_output = ActivationFunctions.relu_derivative(x)\n",
    "\n",
    "# ✅ Test Sigmoid\n",
    "sigmoid_output = ActivationFunctions.sigmoid(x)\n",
    "sigmoid_derivative_output = ActivationFunctions.sigmoid_derivative(x)\n",
    "\n",
    "# ✅ Test Tanh\n",
    "tanh_output = ActivationFunctions.tanh(x)\n",
    "tanh_derivative_output = ActivationFunctions.tanh_derivative(x)\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"Input:\", x)\n",
    "print(\"ReLU Output:\", relu_output)\n",
    "print(\"ReLU Derivative:\", relu_derivative_output)\n",
    "print(\"Sigmoid Output:\", sigmoid_output)\n",
    "print(\"Sigmoid Derivative:\", sigmoid_derivative_output)\n",
    "print(\"tanh Output:\", tanh_output)\n",
    "print(\"tanh Derivative:\", tanh_derivative_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp  # Assuming you're using CuPy for GPU acceleration\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 initializer: str = \"he\"):\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights\n",
    "        if initializer == 'he':\n",
    "            scale = cp.sqrt(2.0 / in_features)  # Fixed denominator\n",
    "        elif initializer == 'xavier':\n",
    "            scale = cp.sqrt(1.0 / in_features)  # Fixed denominator\n",
    "        else:  # Plain initialization\n",
    "            scale = 1.0\n",
    "\n",
    "        self.weights = cp.random.randn(out_features, in_features) * scale\n",
    "        self.bias = cp.zeros((out_features,)) if bias else None\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dweights = cp.zeros_like(self.weights)\n",
    "        self.dbias = cp.zeros_like(self.bias) if bias else None\n",
    "\n",
    "    def forward(self, x: cp.ndarray) -> cp.ndarray:\n",
    "        self.x = x \n",
    "        return cp.dot(x, self.weights.T) + (self.bias if self.bias is not None else 0)  # Ensures correct shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.32285007 -0.58949284]\n",
      " [-1.8907843  -2.54665149]\n",
      " [-2.45871853 -4.50381014]\n",
      " [-3.02665276 -6.46096879]]\n"
     ]
    }
   ],
   "source": [
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "            [10, 11, 12]])\n",
    "\n",
    "layer = Linear(3, 2)  # 3 input features, 2 output features\n",
    "out  = layer.forward(x)\n",
    "\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, initializer: str = \"he\"):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize weights\n",
    "        if initializer == 'he':\n",
    "            scale = cp.sqrt(2.0 / in_features)\n",
    "        elif initializer == 'xavier':\n",
    "            scale = cp.sqrt(1.0 / in_features)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        self.weights = cp.random.randn(out_features, in_features) * scale\n",
    "        self.bias = cp.zeros((out_features,)) if bias else None\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dweights = cp.zeros_like(self.weights)\n",
    "        self.dbias = cp.zeros_like(self.bias) if bias else None\n",
    "\n",
    "    def forward(self, x: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\" Forward pass: Computes Y = XW^T + b \"\"\"\n",
    "        self.x = x  # Store input for backprop\n",
    "        return cp.dot(x, self.weights.T) + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "    def backward(self, upstream_grad: cp.ndarray) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation)\n",
    "\n",
    "        Args:\n",
    "            upstream_grad: Gradient from subsequent layer, shape (batch_size, output_dim)\n",
    "\n",
    "        Returns:\n",
    "            Gradient with respect to input, shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        # Compute gradients\n",
    "        self.dweights = cp.dot(upstream_grad.T, self.x)  # (out_features, in_features)\n",
    "        if self.bias is not None:\n",
    "            self.dbias = cp.sum(upstream_grad, axis=0)    # (out_features,)\n",
    "\n",
    "        # Compute gradient for input\n",
    "        dx = cp.dot(upstream_grad, self.weights)         # (batch_size, in_features)\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update(self, learning_rate: float):\n",
    "        \"\"\"Update weights using computed gradients\"\"\"\n",
    "        self.weights -= learning_rate * self.dweights\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate * self.dbias\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\"Return weights and biases\"\"\"\n",
    "        return {'weights': self.weights, 'bias': self.bias}\n",
    "\n",
    "    @property\n",
    "    def gradients(self):\n",
    "        \"\"\"Return current gradients\"\"\"\n",
    "        return {'dweights': self.dweights, 'dbias': self.dbias}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      " [[ -6.19577508   2.3351226 ]\n",
      " [-14.32964619   6.19233303]\n",
      " [-22.46351731  10.04954346]\n",
      " [-30.59738842  13.90675389]]\n",
      "\n",
      "Gradient w.r.t Input:\n",
      " [[-1.14042808 -1.28208691 -1.0282569 ]\n",
      " [-0.37664654 -0.58789628 -0.47720214]\n",
      " [ 1.59104384  0.88767229  0.68070948]\n",
      " [-0.01415871  0.74768519  0.62611587]]\n",
      "\n",
      "Weight Gradient:\n",
      " [[-3.62594284 -3.36871256 -3.11148227]\n",
      " [12.96040288 13.21238065 13.46435842]]\n",
      "\n",
      "Bias Gradient:\n",
      " [0.25723028 0.25197777]\n",
      "\n",
      "Updated Weights:\n",
      " [[-0.27652006 -1.27884993 -1.054859  ]\n",
      " [ 0.4270596   0.27663676  0.18566903]]\n",
      "\n",
      "Updated Bias:\n",
      " [-0.0025723  -0.00251978]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3) -> batch_size=4, input_dim=3\n",
    "\n",
    "# Create Linear layer: input_dim=3, output_dim=2\n",
    "linear_layer = Linear(in_features=3, out_features=2, bias=True)\n",
    "\n",
    "# Forward pass\n",
    "output = linear_layer.forward(x)\n",
    "print(\"Forward Output:\\n\", output)\n",
    "\n",
    "# Define upstream gradient (random values for testing)\n",
    "upstream_grad = cp.random.randn(4, 2)  # Same shape as output\n",
    "\n",
    "# Backward pass\n",
    "grad_input = linear_layer.backward(upstream_grad)\n",
    "print(\"\\nGradient w.r.t Input:\\n\", grad_input)\n",
    "\n",
    "# Print gradients of weights and bias\n",
    "print(\"\\nWeight Gradient:\\n\", linear_layer.dweights)\n",
    "print(\"\\nBias Gradient:\\n\", linear_layer.dbias)\n",
    "\n",
    "# Update parameters\n",
    "learning_rate = 0.01\n",
    "linear_layer.update(learning_rate)\n",
    "\n",
    "# Print updated weights and bias\n",
    "print(\"\\nUpdated Weights:\\n\", linear_layer.weights)\n",
    "print(\"\\nUpdated Bias:\\n\", linear_layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, activation: str):\n",
    "        self.activation = activation\n",
    "        self.x = None  # Store input for backprop\n",
    "\n",
    "        # Set activation function and its derivative\n",
    "        if activation == \"relu\":\n",
    "            self.func = ActivationFunctions.relu\n",
    "            self.derivative = ActivationFunctions.relu_derivative\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.func = ActivationFunctions.sigmoid\n",
    "            self.derivative = ActivationFunctions.sigmoid_derivative\n",
    "        elif activation == \"tanh\":\n",
    "            self.func = ActivationFunctions.tanh\n",
    "            self.derivative = ActivationFunctions.tanh_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass: Apply activation function \"\"\"\n",
    "        self.x = x  # Store for backward pass\n",
    "        return self.func(x)\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        \"\"\" Backward pass: Apply activation derivative \"\"\"\n",
    "        return upstream_grad * self.derivative(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated Output:\n",
      " [[ 0.9998911  -0.99980217]\n",
      " [ 1.         -1.        ]\n",
      " [ 1.         -1.        ]\n",
      " [ 1.         -1.        ]]\n",
      "\n",
      "Gradient w.r.t Input:\n",
      " [[-5.89217561e-04  2.08099719e-04 -6.22952602e-04]\n",
      " [-7.44122715e-11 -1.65746782e-10 -1.07999116e-10]\n",
      " [-2.36778475e-16  2.05251102e-16 -2.31760313e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3)\n",
    "\n",
    "# Create layers\n",
    "linear_layer = Linear(in_features=3, out_features=2, bias=True)\n",
    "activation_layer = Activation(\"tanh\")  # Can be \"sigmoid\" or \"tanh\"\n",
    "\n",
    "# Forward pass\n",
    "linear_output = linear_layer.forward(x)\n",
    "activated_output = activation_layer.forward(linear_output)\n",
    "\n",
    "print(\"Activated Output:\\n\", activated_output)\n",
    "\n",
    "# Backward pass (simulating loss gradient)\n",
    "upstream_grad = cp.random.randn(4, 2)\n",
    "grad_activated = activation_layer.backward(upstream_grad)\n",
    "grad_input = linear_layer.backward(grad_activated)\n",
    "\n",
    "print(\"\\nGradient w.r.t Input:\\n\", grad_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"\n",
    "        A simple sequential model to stack layers.\n",
    "\n",
    "        Args:\n",
    "            *layers: A list of layers (Linear, Activation, etc.)\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through all layers \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        \"\"\" Backward pass through all layers in reverse order \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            upstream_grad = layer.backward(upstream_grad)\n",
    "        return upstream_grad\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\" Update weights of layers that have parameters (Linear layers) \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"update\"):\n",
    "                layer.update(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output:\n",
      " "
     ]
    },
    {
     "ename": "CUDARuntimeError",
     "evalue": "cudaErrorLaunchFailure: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDARuntimeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel Output:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Backward pass (simulating loss gradient)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m upstream_grad \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1747\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__str__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1863\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:586\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPointer.copy_to_host_async\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy_backends/cuda/api/runtime.pyx:606\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.runtime.memcpyAsync\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy_backends/cuda/api/runtime.pyx:146\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.runtime.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCUDARuntimeError\u001b[0m: cudaErrorLaunchFailure: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# Define input\n",
    "x = cp.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])  # (4, 3)\n",
    "\n",
    "# Create a simple feedforward network\n",
    "model = Sequential(\n",
    "    Linear(in_features=3, out_features=5, bias=True),\n",
    "    Activation(\"relu\"),\n",
    "    Linear(in_features=5, out_features=2, bias=True),\n",
    "    Activation(\"sigmoid\")\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Model Output:\\n\", output)\n",
    "\n",
    "# Backward pass (simulating loss gradient)\n",
    "upstream_grad = cp.random.randn(4, 2)\n",
    "model.backward(upstream_grad)\n",
    "\n",
    "# Update weights\n",
    "model.update(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SilvaXnet_cuda11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
