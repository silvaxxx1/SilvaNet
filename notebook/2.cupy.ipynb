{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp \n",
    "\n",
    "x = cp.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9]])\n",
    "\n",
    "y = cp.array([1,0,0])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 26 02:21:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   49C    P3             13W /   30W |      65MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5638      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     12250      C   ...a3/envs/SilvaXnet_cuda11/bin/python         56MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.35713845e-01  9.35595471e-01  1.07025822e+00 -1.78587871e-01\n",
      "   7.93042990e-01  1.68280696e-03]\n",
      " [-1.09309289e+00 -1.36785874e+00  3.62140521e-01  1.86794945e+00\n",
      "  -6.78621715e-01  2.15288539e-01]\n",
      " [-1.79336067e-01  1.61288816e+00 -1.23014474e+00  9.78876973e-01\n",
      "   1.33692201e+00 -1.26535825e+00]]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "(3, 6)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "# lets decide the number of hiiden neurons we want\n",
    "# we only we use on hidden  layer\n",
    "n_neurons = 6\n",
    "\n",
    "num_of_example , num_of_features = x.shape \n",
    "\n",
    "# initialize the weights and biases\n",
    "W1 = cp.random.randn(num_of_features, n_neurons)\n",
    "b1 = cp.zeros((n_neurons,)) \n",
    "\n",
    "print(W1)\n",
    "print(b1)\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.95990783   3.03854246  -1.89589496   6.49394195   3.44656558\n",
      "   -3.36381486]\n",
      " [ -7.48433624   6.58041712  -1.28913296  14.4986576    7.80059543\n",
      "   -6.50897557]\n",
      " [-12.00876465  10.12229178  -0.68237096  22.50337326  12.15462528\n",
      "   -9.65413628]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "# the forward pass (linear transformation)\n",
    "\n",
    "z1 = cp.dot(x, W1) + b1\n",
    "\n",
    "print(z1)\n",
    "print(z1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.994643    0.9954208  -0.95588464  0.99999542  0.99797259 -0.99760813]\n",
      " [-0.99999937  0.99999615 -0.85889929  1.          0.99999966 -0.99999556]\n",
      " [-1.          1.         -0.59305861  1.          1.         -0.99999999]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "# activation function\n",
    "## adding the non linearity to the linear transformation\n",
    "\n",
    "a1 = cp.tanh(z1)\n",
    "\n",
    "print(a1)\n",
    "print(a1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77732016]\n",
      " [ 0.40734385]\n",
      " [ 0.61643084]\n",
      " [-0.22694915]\n",
      " [-0.15064317]\n",
      " [-0.32521864]]\n",
      "[0.]\n",
      "(6, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# the output of the network\n",
    "out_layer_neurons = 1\n",
    "W2 = cp.random.randn(n_neurons, out_layer_neurons)\n",
    "b2 = cp.zeros((out_layer_neurons,))\n",
    "\n",
    "print(W2)\n",
    "print(b2)\n",
    "print(W2.shape)\n",
    "print(b2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53655271]\n",
      " [0.60283487]\n",
      " [0.76671071]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# second forward pass\n",
    "z2 = cp.dot(a1 , W2) + b2 \n",
    "print(z2)\n",
    "print(z2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63101013]\n",
      " [0.64630461]\n",
      " [0.68280893]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## adding the non linearity to the linear transformation \n",
    "## this type we will use sigmoid activation function since this is a binary classification problem \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(a2)\n",
    "print(a2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8497300432839547\n"
     ]
    }
   ],
   "source": [
    "## next we must calculate the loss\n",
    "## we will use binary cross entropy loss function , again becuse this is a binary classification problem\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = cp.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -cp.mean(y_true * cp.log(y_pred) + (1 - y_true) * cp.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.58476062  2.71010148  2.71010148]\n",
      " [-1.54725803  2.82729159  2.82729159]\n",
      " [-1.46453856  3.15267383  3.15267383]]\n"
     ]
    }
   ],
   "source": [
    "## now we must optimize (minmize this loss function) by the power of chain rule and gradient descent \n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = cp.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -((y_true / y_pred) - ((1 - y_true) / (1 - y_pred)))\n",
    "\n",
    "d_loss_d_a2 = binary_cross_entropy_derivative(y, a2)\n",
    "print(d_loss_d_a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23283635]\n",
      " [0.22859496]\n",
      " [0.2165809 ]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x*(1 - x) \n",
    "\n",
    "d_a2_d_z2 = sigmoid_derivative(a2)\n",
    "\n",
    "print(d_a2_d_z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36898987  0.63101013  0.63101013]\n",
      " [-0.35369539  0.64630461  0.64630461]\n",
      " [-0.31719107  0.68280893  0.68280893]]\n"
     ]
    }
   ],
   "source": [
    "out_layer_grad = d_loss_d_a2 * d_a2_d_z2 \n",
    "print(out_layer_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. a2: [[-0.36898987]\n",
      " [ 0.64630461]\n",
      " [ 0.68280893]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## TO simply we can use the this function and get the same result :\n",
    "\n",
    "def out_layer_grad(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    \n",
    "    y_pred = cp.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return (y_pred - y_true)\n",
    "\n",
    "# Reshape y to match a2\n",
    "y = y.reshape(-1, 1)  # Ensure y has shape (3,1)\n",
    "\n",
    "# Compute the gradient of the loss with respect to a2\n",
    "grad_a2 = out_layer_grad(y, a2)\n",
    "\n",
    "print(\"Gradient w.r.t. a2:\", grad_a2)\n",
    "print(grad_a2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25379174]\n",
      " [ 0.43692618]\n",
      " [ 0.44242244]]\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "## lets move to the backprop of the hidden layer:\n",
    "def tanh_derivative(x):\n",
    "    return 1 - cp.tanh(x)**2 \n",
    "\n",
    "\n",
    "grad_z2 = tanh_derivative(a2) * grad_a2\n",
    "\n",
    "print(grad_z2)\n",
    "print(grad_z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20897206]\n",
      " [ 0.20890579]\n",
      " [-0.13168747]\n",
      " [ 0.20851935]\n",
      " [ 0.20869043]\n",
      " [-0.20872066]]\n",
      "(6, 1)\n",
      "[[0.20851896]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# now we need  to calcukate the grad for the parametrs in the hidden layer (W2 , B2)\n",
    "dW2 = cp.dot( a1.T, grad_z2) / num_of_example\n",
    "db2 = cp.sum(grad_z2, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "print(dW2)\n",
    "print(dW2.shape)\n",
    "\n",
    "print(db2)\n",
    "print(db2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19727744 -0.10338051 -0.15644506  0.05759782  0.03823199  0.0825378 ]\n",
      " [-0.33963153  0.17797919  0.26933477 -0.09916002 -0.06581995 -0.14209654]\n",
      " [-0.34390389  0.18021806  0.27272284 -0.1004074  -0.06664792 -0.14388402]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "grad_a1 = cp.dot(grad_z2, W2.T)\n",
    "\n",
    "print(grad_a1)\n",
    "print(grad_a1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08352926 -0.04372067 -0.07021089  0.02418977  0.01610609  0.0347902 ]\n",
      " [-0.14263667  0.07474713  0.13898085 -0.04164467 -0.0276427  -0.0596773 ]\n",
      " [-0.14443081  0.07568696  0.19550908 -0.04216853 -0.02799042 -0.0604276 ]]\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "dz1  = grad_a1 * tanh_derivative(a1)\n",
    "print(dz1)\n",
    "print(dz1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20897206]\n",
      " [ 0.20890579]\n",
      " [-0.13168747]\n",
      " [ 0.20851935]\n",
      " [ 0.20869043]\n",
      " [-0.20872066]]\n",
      "(6, 1)\n",
      "[[0.20851896]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# now we need  to calcukate the grad for the parametrs in the hidden layer (W2 , B2)\n",
    "dW1 = cp.dot( x.T, dz1) / num_of_example\n",
    "db1 = cp.sum(dz1, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "print(dW2)\n",
    "print(dW2.shape)\n",
    "\n",
    "print(db2)\n",
    "print(db2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new W1: [[-0.2307204   0.93297855  1.0640773  -0.17712931  0.79401098  0.00377251]\n",
      " [-1.08742099 -1.37083137  0.35507867  1.86960676 -0.67752197  0.21766263]\n",
      " [-0.1729857   1.60955981 -1.23808752  0.98073302  1.33815351 -1.26269978]]\n",
      "new b1: [ 0.00067846 -0.00035571 -0.00088093  0.00019874  0.00013176  0.00028438]\n",
      "new W2: [[-0.77523044]\n",
      " [ 0.4052548 ]\n",
      " [ 0.61774772]\n",
      " [-0.22903434]\n",
      " [-0.15273008]\n",
      " [-0.32313143]]\n",
      "new bw: [-0.00208519]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "\n",
    "W1 -= lr * dW1\n",
    "b1 -= lr * db1.ravel()  # Fix here\n",
    "W2 -= lr * dW2\n",
    "b2 -= lr * db2.ravel()  # No need to fix since b2 is already (1,)\n",
    "\n",
    "\n",
    "print(\"new W1:\", W1)\n",
    "print(\"new b1:\", b1)\n",
    "print(\"new W2:\", W2)\n",
    "print(\"new bw:\", b2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.8614092666656665\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "z1 = cp.dot(x, W1) + b1\n",
    "a1 = cp.tanh(z1)\n",
    "\n",
    "z2 = cp.dot(a1, W2) + b2\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "# Compute new loss\n",
    "loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "print(\"New loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7050632553774739\n",
      "Epoch 10, Loss: 0.7037360046511018\n",
      "Epoch 20, Loss: 0.7025508787021367\n",
      "Epoch 30, Loss: 0.7014925155240381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.7005471705657625\n",
      "Epoch 50, Loss: 0.6997025583526734\n",
      "Epoch 60, Loss: 0.6989477068245045\n",
      "Epoch 70, Loss: 0.6982728239990537\n",
      "Epoch 80, Loss: 0.697669176421448\n",
      "Epoch 90, Loss: 0.6971289787579569\n",
      "Epoch 100, Loss: 0.6966452938311406\n",
      "Epoch 110, Loss: 0.6962119423614923\n",
      "Epoch 120, Loss: 0.6958234216727721\n",
      "Epoch 130, Loss: 0.6954748326281223\n",
      "Epoch 140, Loss: 0.6951618140870238\n",
      "Epoch 150, Loss: 0.6948804842053002\n",
      "Epoch 160, Loss: 0.6946273879385744\n",
      "Epoch 170, Loss: 0.6943994501513866\n",
      "Epoch 180, Loss: 0.6941939337777024\n",
      "Epoch 190, Loss: 0.6940084025223086\n",
      "Epoch 200, Loss: 0.6938406876356007\n",
      "Epoch 210, Loss: 0.6936888583357151\n",
      "Epoch 220, Loss: 0.6935511954913448\n",
      "Epoch 230, Loss: 0.6934261682156063\n",
      "Epoch 240, Loss: 0.6933124130557684\n",
      "Epoch 250, Loss: 0.6932087154954724\n",
      "Epoch 260, Loss: 0.6931139935152831\n",
      "Epoch 270, Loss: 0.6930272829840494\n",
      "Epoch 280, Loss: 0.6929477246777714\n",
      "Epoch 290, Loss: 0.6928745527445644\n",
      "Epoch 300, Loss: 0.6928070844540737\n",
      "Epoch 310, Loss: 0.6927447110874381\n",
      "Epoch 320, Loss: 0.6926868898398412\n",
      "Epoch 330, Loss: 0.6926331366219189\n",
      "Epoch 340, Loss: 0.6925830196590411\n",
      "Epoch 350, Loss: 0.6925361537988156\n",
      "Epoch 360, Loss: 0.6924921954472811\n",
      "Epoch 370, Loss: 0.6924508380632408\n",
      "Epoch 380, Loss: 0.6924118081481684\n",
      "Epoch 390, Loss: 0.6923748616762232\n",
      "Epoch 400, Loss: 0.6923397809151848\n",
      "Epoch 410, Loss: 0.6923063715947064\n",
      "Epoch 420, Loss: 0.6922744603832292\n",
      "Epoch 430, Loss: 0.6922438926392683\n",
      "Epoch 440, Loss: 0.6922145304066776\n",
      "Epoch 450, Loss: 0.6921862506269183\n",
      "Epoch 460, Loss: 0.6921589435444104\n",
      "Epoch 470, Loss: 0.6921325112837345\n",
      "Epoch 480, Loss: 0.6921068665798354\n",
      "Epoch 490, Loss: 0.692081931644498\n",
      "Epoch 500, Loss: 0.692057637154224\n",
      "Epoch 510, Loss: 0.6920339213463117\n",
      "Epoch 520, Loss: 0.6920107292113952\n",
      "Epoch 530, Loss: 0.6919880117720074\n",
      "Epoch 540, Loss: 0.6919657254378867\n",
      "Epoch 550, Loss: 0.691943831429763\n",
      "Epoch 560, Loss: 0.691922295264277\n",
      "Epoch 570, Loss: 0.6919010862934798\n",
      "Epoch 580, Loss: 0.6918801772930865\n",
      "Epoch 590, Loss: 0.6918595440942834\n",
      "Epoch 600, Loss: 0.6918391652544583\n",
      "Epoch 610, Loss: 0.6918190217627229\n",
      "Epoch 620, Loss: 0.6917990967765397\n",
      "Epoch 630, Loss: 0.691779375386167\n",
      "Epoch 640, Loss: 0.6917598444039847\n",
      "Epoch 650, Loss: 0.6917404921760801\n",
      "Epoch 660, Loss: 0.6917213084137499\n",
      "Epoch 670, Loss: 0.6917022840428261\n",
      "Epoch 680, Loss: 0.6916834110689575\n",
      "Epoch 690, Loss: 0.6916646824571708\n",
      "Epoch 700, Loss: 0.691646092024216\n",
      "Epoch 710, Loss: 0.6916276343423609\n",
      "Epoch 720, Loss: 0.6916093046534324\n",
      "Epoch 730, Loss: 0.6915910987920397\n",
      "Epoch 740, Loss: 0.6915730131170106\n",
      "Epoch 750, Loss: 0.6915550444501917\n",
      "Epoch 760, Loss: 0.6915371900218367\n",
      "Epoch 770, Loss: 0.6915194474218951\n",
      "Epoch 780, Loss: 0.6915018145565862\n",
      "Epoch 790, Loss: 0.691484289609702\n",
      "Epoch 800, Loss: 0.6914668710081462\n",
      "Epoch 810, Loss: 0.6914495573912631\n",
      "Epoch 820, Loss: 0.6914323475835598\n",
      "Epoch 830, Loss: 0.6914152405704624\n",
      "Epoch 840, Loss: 0.6913982354767916\n",
      "Epoch 850, Loss: 0.6913813315476632\n",
      "Epoch 860, Loss: 0.6913645281315639\n",
      "Epoch 870, Loss: 0.6913478246653668\n",
      "Epoch 880, Loss: 0.6913312206610833\n",
      "Epoch 890, Loss: 0.691314715694164\n",
      "Epoch 900, Loss: 0.691298309393185\n",
      "Epoch 910, Loss: 0.6912820014307673\n",
      "Epoch 920, Loss: 0.6912657915156001\n",
      "Epoch 930, Loss: 0.6912496793854453\n",
      "Epoch 940, Loss: 0.6912336648010189\n",
      "Epoch 950, Loss: 0.6912177475406497\n",
      "Epoch 960, Loss: 0.6912019273956348\n",
      "Epoch 970, Loss: 0.6911862041662079\n",
      "Epoch 980, Loss: 0.6911705776580609\n",
      "Epoch 990, Loss: 0.6911550476793464\n",
      "Total Training Time: 3.9403 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cupy as cp\n",
    "\n",
    "# Set up synthetic dataset on GPU\n",
    "num_of_example = 1000\n",
    "x = cp.random.randn(num_of_example, 3)\n",
    "y = (cp.random.rand(num_of_example, 1) > 0.5).astype(cp.float32)\n",
    "\n",
    "# Define sigmoid and loss functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -cp.mean(y_true * cp.log(y_pred) + (1 - y_true) * cp.log(1 - y_pred))\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - cp.tanh(x) ** 2\n",
    "\n",
    "def out_layer_grad(y_true, y_pred):\n",
    "    return -(y_true / y_pred - (1 - y_true) / (1 - y_pred))\n",
    "\n",
    "# Initialize weights and biases on GPU\n",
    "W1 = cp.random.randn(3, 6)\n",
    "b1 = cp.zeros((6,))\n",
    "W2 = cp.random.randn(6, 1)\n",
    "b2 = cp.zeros((1,))\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 1000  # Number of training iterations\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = cp.dot(x, W1) + b1\n",
    "    a1 = cp.tanh(z1)\n",
    "\n",
    "    z2 = cp.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y, a2)\n",
    "\n",
    "    # Backpropagation\n",
    "    grad_a2 = out_layer_grad(y, a2)\n",
    "    grad_z2 = grad_a2 * a2 * (1 - a2)  # More efficient sigmoid derivative\n",
    "\n",
    "    dW2 = cp.dot(a1.T, grad_z2) / num_of_example\n",
    "    db2 = cp.sum(grad_z2, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "    grad_z1 = cp.dot(grad_z2, W2.T) * tanh_derivative(a1)\n",
    "\n",
    "    dW1 = cp.dot(x.T, grad_z1) / num_of_example\n",
    "    db1 = cp.sum(grad_z1, axis=0, keepdims=True) / num_of_example\n",
    "\n",
    "    # Parameter updates\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1.ravel()\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2.ravel()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.get()}\")  # Move loss to CPU\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Total Training Time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8592\n",
      "Epoch 10, Loss: 0.8459\n",
      "Epoch 20, Loss: 0.8338\n",
      "Epoch 30, Loss: 0.8229\n",
      "Epoch 40, Loss: 0.8129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.8038\n",
      "Epoch 60, Loss: 0.7956\n",
      "Epoch 70, Loss: 0.7882\n",
      "Epoch 80, Loss: 0.7815\n",
      "Epoch 90, Loss: 0.7754\n",
      "Epoch 100, Loss: 0.7698\n",
      "Epoch 110, Loss: 0.7648\n",
      "Epoch 120, Loss: 0.7603\n",
      "Epoch 130, Loss: 0.7562\n",
      "Epoch 140, Loss: 0.7524\n",
      "Epoch 150, Loss: 0.7490\n",
      "Epoch 160, Loss: 0.7459\n",
      "Epoch 170, Loss: 0.7431\n",
      "Epoch 180, Loss: 0.7405\n",
      "Epoch 190, Loss: 0.7382\n",
      "Epoch 200, Loss: 0.7361\n",
      "Epoch 210, Loss: 0.7341\n",
      "Epoch 220, Loss: 0.7323\n",
      "Epoch 230, Loss: 0.7306\n",
      "Epoch 240, Loss: 0.7291\n",
      "Epoch 250, Loss: 0.7277\n",
      "Epoch 260, Loss: 0.7264\n",
      "Epoch 270, Loss: 0.7252\n",
      "Epoch 280, Loss: 0.7241\n",
      "Epoch 290, Loss: 0.7231\n",
      "Epoch 300, Loss: 0.7222\n",
      "Epoch 310, Loss: 0.7213\n",
      "Epoch 320, Loss: 0.7205\n",
      "Epoch 330, Loss: 0.7197\n",
      "Epoch 340, Loss: 0.7190\n",
      "Epoch 350, Loss: 0.7183\n",
      "Epoch 360, Loss: 0.7177\n",
      "Epoch 370, Loss: 0.7171\n",
      "Epoch 380, Loss: 0.7165\n",
      "Epoch 390, Loss: 0.7160\n",
      "Epoch 400, Loss: 0.7155\n",
      "Epoch 410, Loss: 0.7151\n",
      "Epoch 420, Loss: 0.7146\n",
      "Epoch 430, Loss: 0.7142\n",
      "Epoch 440, Loss: 0.7138\n",
      "Epoch 450, Loss: 0.7134\n",
      "Epoch 460, Loss: 0.7130\n",
      "Epoch 470, Loss: 0.7127\n",
      "Epoch 480, Loss: 0.7123\n",
      "Epoch 490, Loss: 0.7120\n",
      "Epoch 500, Loss: 0.7117\n",
      "Epoch 510, Loss: 0.7114\n",
      "Epoch 520, Loss: 0.7111\n",
      "Epoch 530, Loss: 0.7108\n",
      "Epoch 540, Loss: 0.7106\n",
      "Epoch 550, Loss: 0.7103\n",
      "Epoch 560, Loss: 0.7101\n",
      "Epoch 570, Loss: 0.7098\n",
      "Epoch 580, Loss: 0.7096\n",
      "Epoch 590, Loss: 0.7093\n",
      "Epoch 600, Loss: 0.7091\n",
      "Epoch 610, Loss: 0.7089\n",
      "Epoch 620, Loss: 0.7087\n",
      "Epoch 630, Loss: 0.7084\n",
      "Epoch 640, Loss: 0.7082\n",
      "Epoch 650, Loss: 0.7080\n",
      "Epoch 660, Loss: 0.7078\n",
      "Epoch 670, Loss: 0.7076\n",
      "Epoch 680, Loss: 0.7074\n",
      "Epoch 690, Loss: 0.7072\n",
      "Epoch 700, Loss: 0.7070\n",
      "Epoch 710, Loss: 0.7069\n",
      "Epoch 720, Loss: 0.7067\n",
      "Epoch 730, Loss: 0.7065\n",
      "Epoch 740, Loss: 0.7063\n",
      "Epoch 750, Loss: 0.7061\n",
      "Epoch 760, Loss: 0.7060\n",
      "Epoch 770, Loss: 0.7058\n",
      "Epoch 780, Loss: 0.7056\n",
      "Epoch 790, Loss: 0.7054\n",
      "Epoch 800, Loss: 0.7053\n",
      "Epoch 810, Loss: 0.7051\n",
      "Epoch 820, Loss: 0.7049\n",
      "Epoch 830, Loss: 0.7048\n",
      "Epoch 840, Loss: 0.7046\n",
      "Epoch 850, Loss: 0.7045\n",
      "Epoch 860, Loss: 0.7043\n",
      "Epoch 870, Loss: 0.7042\n",
      "Epoch 880, Loss: 0.7040\n",
      "Epoch 890, Loss: 0.7038\n",
      "Epoch 900, Loss: 0.7037\n",
      "Epoch 910, Loss: 0.7035\n",
      "Epoch 920, Loss: 0.7034\n",
      "Epoch 930, Loss: 0.7033\n",
      "Epoch 940, Loss: 0.7031\n",
      "Epoch 950, Loss: 0.7030\n",
      "Epoch 960, Loss: 0.7028\n",
      "Epoch 970, Loss: 0.7027\n",
      "Epoch 980, Loss: 0.7025\n",
      "Epoch 990, Loss: 0.7024\n",
      "Total Training Time: 4.8838 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate synthetic dataset\n",
    "num_of_example = 1000\n",
    "x = np.random.randn(num_of_example, 3)\n",
    "y = (np.random.rand(num_of_example, 1) > 0.5).astype(np.float32)\n",
    "\n",
    "# Convert input data to tensors and move to device\n",
    "tensor_x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "tensor_y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define model\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 6)  # input: 3, output: 6\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(6, 1)  # input: 6, output: 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.fc1(x)\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2 \n",
    "\n",
    "# Initialize model and move to device\n",
    "model = MyNet().to(device)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define weights and biases\n",
    "W1 = np.random.randn(3, 6)\n",
    "b1 = np.zeros((6,))\n",
    "W2 = np.random.randn(6, 1)\n",
    "b2 = np.zeros((1,))\n",
    "\n",
    "# Assign weights and biases, ensuring they're on the same device\n",
    "model.fc1.weight.data = torch.tensor(W1.T, dtype=torch.float32, device=device)\n",
    "model.fc1.bias.data = torch.tensor(b1, dtype=torch.float32, device=device)\n",
    "model.fc2.weight.data = torch.tensor(W2.T, dtype=torch.float32, device=device)\n",
    "model.fc2.bias.data = torch.tensor(b2, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "try:\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()   # Reset gradients\n",
    "        output = model(tensor_x)  # Forward pass\n",
    "        loss = criterion(output, tensor_y)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\", flush=True)\n",
    "\n",
    "    # Ensure GPU operations finish before stopping timer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Total Training Time: {end_time - start_time:.4f} seconds\", flush=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SilvaXnet_cuda11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
